---
title: "wk6-prac"
author: "Leandra"
date: "11/17/2021"
output: html_document
---

```{r message=FALSE}
library(spatstat)
library(here)
library(janitor)
library(sp)
library(sf)
library(geojson)
library(geojsonio)
library(tmap)
library(tmaptools)
library(tidyverse)
library(raster)
library(fpc) #for DBSCAN analysis
library(dbscan)
```

```{r London borough boundaries}
LondonBoroughs <- st_read(here::here("../", "wk1", "statistical-gis-boundaries-london", "ESRI", "London_Borough_Excluding_MHW.shp"))

BoroughMap <- LondonBoroughs %>%
  dplyr::filter(str_detect(GSS_CODE, "^E09"))%>%
  st_transform(., 27700)

qtm(BoroughMap)

summary(BoroughMap)
```

```{r Blue plaques}
BluePlaques <- st_read("https://s3.eu-west-2.amazonaws.com/openplaques/open-plaques-london-2018-04-08.geojson") %>%
  st_transform(., 27700)

summary(BluePlaques)

tmap_mode("plot")

tm_shape(BoroughMap) +
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(BluePlaques) +
  tm_dots(col = "blue")
```

```{r Distinct blue plaques}
#remove duplicates
BluePlaques <- BluePlaques %>%
  distinct(geometry, .keep_all=TRUE)
```

```{r Spatial subsetting}
BluePlaquesSub <- BluePlaques[BoroughMap,]

#check to see that they've been removed
tm_shape(BoroughMap) +
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")

```

```{r Filter Harrow}
Harrow <- BoroughMap %>%
  filter(., NAME=="Harrow")

#Check to see that the correct borough has been pulled out
tm_shape(Harrow) +
  tm_polygons(col = NA, alpha = 0.5)

#clip the data to our single borough
BluePlaquesSub <- BluePlaques[Harrow,]

#check that it's worked
tm_shape(Harrow) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")

```

```{r Set up for spatstat}
#set an observation window for spatstat to carry out analysis within
#set this as the borough boundary
window <- as.owin(Harrow)
plot(window)

#create a ppp object
#first make our sf object to sp object (the original spatial package in r)
BluePlaquesSub<- BluePlaquesSub %>%
  as(., 'Spatial')

#then from sp to ppp
BluePlaquesSub.ppp <- ppp(x=BluePlaquesSub@coords[,1],
                          y=BluePlaquesSub@coords[,2],
                          window=window)

BluePlaquesSub.ppp %>%
  plot(.,pch=16,cex=0.5, 
       main="Blue Plaques Harrow")
```

```{r Kernel Density Estimation}
BluePlaquesSub.ppp %>%
  density(., sigma=500) %>%
  plot(main="Kernel Density Estimation")
#The sigma value sets the diameter of the Kernel (in the units your map is in — in this case, as we are in British National Grid the units are in metres).

BluePlaquesSub.ppp %>%
  density(., sigma=1000) %>%
  plot(main="Kernel Density Estimation")
```

Quadrat analysis is a method to check whether the distribution of points in our study area differs from complete spatial randomness.

```{r Quadrat Analysis Map}
#First plot the points
plot(BluePlaquesSub.ppp,
     pch=16,
     cex=0.5, 
     main="Blue Plaques in Harrow")

#now count the points in that fall in a 6 x 6
#grid overlaid across the map
BluePlaquesSub.ppp %>%
  quadratcount(., nx = 6, ny = 6)%>%
  plot(., add=T, col="red")

```

```{r Quadrat Count}
Qcount <- BluePlaquesSub.ppp %>%
  quadratcount(., nx = 6, ny = 6) %>%
  as.data.frame() %>%
  dplyr::count(Var1=Freq)%>%
  dplyr::rename(Freqquadratcount=n)

Qcount %>%
  summarise_all(class)
```

```{r Calculate expected values}
sums <- Qcount %>%
  #calculate the total blue plaques (Var * Freq)
  mutate(total = Var1 * Freqquadratcount) %>%
  dplyr::summarise(across(everything(), sum)) %>%
  dplyr::select(-Var1) 
  #keep Freqquadratcount for the QCountTable later

#calculate lambda
lambda <- Qcount%>%
  mutate(total = Var1 * Freqquadratcount) %>%
  dplyr::summarise(across(everything(), sum)) %>%
  mutate(lambda = total/Freqquadratcount) %>%
  dplyr::select(lambda) %>%
  pull(lambda) #pull out the single value

#formula for calculating expected probabilities based on the Poisson distribution
QCountTable <- Qcount %>%
  mutate(Pr=((lambda^Var1)*exp(-lambda))/factorial(Var1)) %>%
  #calculate the expected counts based on our total number of plaques
  #and save them to the table
  mutate(Expected=(round(Pr * sums$Freqquadratcount, 0)))
```

```{r Plot observed vs expected}
#Compare the frequency distributions of the observed and expected point patterns
plot(c(1,5),c(0,14), type="n",
     xlab="Number of Blue Plaques (Red=Observed,Blue=Expected)", 
     ylab="Frequency of Occurances")
points(QCountTable$Freqquadratcount, 
       col="Red", 
       type="o", 
       lwd=3)
points(QCountTable$Expected, col="Blue", 
       type="o", 
       lwd=3)
```

```{r Chi-squared Test}
teststats <- quadrat.test(BluePlaquesSub.ppp, nx = 6, ny = 6)

plot(BluePlaquesSub.ppp, pch=16, cex=0.5, main="Blue Plaques in Harrow")
plot(teststats, add=T, col = "red")

# residuals(teststats)
```
The quadrat.test() function from spatstat uses a Chi-squared test to compare the observed and expected frequencies for each quadrat (rather than for quadrat bins, as we have computed above).
* Top left value in each quadrat is the observed value
* Top right is the expected value -- notice how full quadrats have a value of 2, smaller quadrats have a smaller value. This is the expected value under CSR.
* An important requirement of the \(\chi^2\) test is that the expected counts in each quadrat be greater than 5.
* Bottom value is the Pearson residual: (obs - exp) / sqrt(exp)

Note on MAUP:
* Modifiable Areal Unit Problem (MAUP) refers to a bias created when the same spatial data yields different results when aggregated into target polygons. 
* Scale effect: major analytical differences when the data is analysed at higher or lower levels of aggregation
* One way to mitigate this is to present data showing multiple scales simultaneously
* Another way is to ensure that we include the scale of the aggregation unit e.g. "at the County level the pattern appears ..."

```{r Ripleys K}
K <- BluePlaquesSub.ppp %>%
  Kest(., correction="border") %>%
  plot()

#line in red is the theoretical value of K for each distance window (r) under a Poisson assumption of Complete Spatial Randomness.
#black line is the estimated values of K accounting for the effects of the edge of the study area.

#Where the value of K (black line) falls above the CSR line, the data appear to be clustered at that distance. Where the value of K is below the line, the data are dispersed.
```

```{r DBSCAN}
#Density-based spatial clustering of applications with noise
#Quadrat and Ripley’s K analysis are useful exploratory techniques for telling us if we have spatial clusters present in our point data, but they are not able to tell us WHERE the clusters are occurring.

st_geometry(BoroughMap)

#first extract the points from the spatial points data frame
BluePlaquesSubPoints <- BluePlaquesSub %>%
  coordinates(.) %>%
  as.data.frame()

#now run the dbscan analysis
db <- BluePlaquesSubPoints %>%
  fpc::dbscan(., eps = 700, MinPts = 4)
#Epsilon: radius within which to search for other points
#eps value can be decided using output from Ripley's K analysis
#MinPts: minimum number of points around the seed to be considered a cluster
#as a spatial analyst you will have to decide the values of these params

#now plot the results
plot(db, BluePlaquesSubPoints, main = "DBSCAN Output", frame = F)
plot(BoroughMap$geometry, add=T)
#plot.dbscan distinguishes between seed and border points by plot symbol.
```

```{r DBSCAN Knee Plot}
# used to find suitable eps value based on the knee in plot
# k is number of nearest neighbours used (use Minpts)

BluePlaquesSubPoints%>%
  dbscan::kNNdistplot(., k=4)

#The knee is visible around a distance of 900?
```

```{r Plot DBSCAN clusters}
db
#info on the number of points belonging to the clusters that are seeds and border points.

db$cluster

BluePlaquesSubPoints<- BluePlaquesSubPoints %>%
  mutate(dbcluster=db$cluster)

#create some convex hull polygons to wrap around the points in our clusters
chulls <- BluePlaquesSubPoints %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(coords.x1, coords.x2)))%>%
  arrange(hull)
#because we want to create something completely new we need to prepare a new empty column - here we give each group a value of 1 to the end point in the cluster - e.g. 5 points in the cluster get values 1 to 5
#then we overwrite that with the hull information, but we need to specify it's a factor
#so the first mutate is simply to make a new column ready for the second mutate

#0 isn’t actually a cluster (it’s all points that aren’t in a cluster) 
#drop it from the dataframe
chulls <- chulls %>%
  filter(dbcluster >=1)

dbplot <- ggplot(data=BluePlaquesSubPoints, 
                 aes(coords.x1,coords.x2, colour=dbcluster, fill=dbcluster)) 
dbplot
#add the points in
dbplot <- dbplot + geom_point()
#now the convex hulls
dbplot <- dbplot + geom_polygon(data = chulls, 
                                aes(coords.x1,coords.x2, group=dbcluster), 
                                alpha = 0.5) 
#now plot, setting the coordinates to scale correctly and as a black and white plot 
dbplot + theme_bw() + coord_equal()

```

```{r warning=FALSE}
#First get the bbox in lat long for Harrow
#when getting basemap from OSM must use WGS84
HarrowWGSbb <- Harrow %>%
  st_transform(., 4326)%>%
  st_bbox()

library(OpenStreetMap)

basemap <- OpenStreetMap::openmap(c(51.5549876,-0.4040502),
                                  c(51.6405356,-0.2671315),
                                  zoom=NULL,
                                  "stamen-toner")

# convert the basemap to British National Grid
basemap_bng <- openproj(basemap, projection="+init=epsg:27700")

#autoplot(basemap_bng) sometimes works
autoplot.OpenStreetMap(basemap_bng)+ 
  geom_point(data=BluePlaquesSubPoints, 
             aes(coords.x1,coords.x2, 
                 colour=dbcluster, 
                 fill=dbcluster)) + 
  geom_polygon(data = chulls, 
               aes(coords.x1,coords.x2, 
                   group=dbcluster,
                   fill=dbcluster), 
               alpha = 0.5)  

```

```{r Spatially referenced continuous observations}
#continuous observations (counts of blue plaques, average GCSE scores, average incomes etc.)
#spatially referenced (i.e. attached to a spatial unit like a ward or a borough)
Londonwards <- st_read(here('London-wards-2018_ESRI', 'London_Ward.shp'))

LondonWardsMerged <- st_read(here::here("../", 
                                        "wk1", 
                                        "statistical-gis-boundaries-london", 
                                        "ESRI",
                                        "London_Ward_CityMerged.shp"))%>%
  st_transform(.,27700)

WardData <- read_csv("https://data.london.gov.uk/download/ward-profiles-and-atlas/772d2d64-e8c6-46cb-86f9-e52b4c7851bc/ward-profiles-excel-version.csv", 
                     na = c("NA", "n/a"), 
                     locale=locale(encoding = "latin1")) %>% 
  clean_names()

LondonWardsMerged <- LondonWardsMerged %>% 
  left_join(WardData, 
            by = c("GSS_CODE" = "new_code"))%>%
  dplyr::distinct(GSS_CODE, .keep_all = T)%>%
  dplyr::select(GSS_CODE, ward_name, average_gcse_capped_point_scores_2014)

#check if CRS is correct
st_crs(LondonWardsMerged)

tmap_mode("plot")
# tm_shape(LondonWardsMerged) +
#   tm_polygons(col = NA, alpha = 0.5) +
#   tm_shape(BluePlaques) +
#   tm_dots(col = "blue")

summary(BluePlaques)

BluePlaquesSub <- BluePlaques[LondonWardsMerged,]

tm_shape(LondonWardsMerged) +
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")
```

```{r Spatial join}
points_sf_joined <- LondonWardsMerged%>%
  st_join(BluePlaquesSub)%>%
  add_count(ward_name)%>%
  janitor::clean_names()%>%
  #calculate area
  mutate(area=st_area(.))%>%
  #then density of the points per ward
  mutate(density=n/area)%>%
  #select density and some other variables 
  dplyr::select(density, ward_name, gss_code, n, average_gcse_capped_point_scores_2014)

points_sf_joined <- points_sf_joined %>%                    
  group_by(gss_code) %>%         
  summarise(density = first(density),
            wardname = first(ward_name),
            plaquecount = first(n))

# tmap_mode("view")
tm_shape(points_sf_joined) +
  tm_polygons("density",
              style="jenks",
              palette="PuOr",
              midpoint=NA,
              popup.vars=c("wardname", "density"),
              title="Blue Plaque Density")
```

```{r Neighbours list}
library(spdep)

#First calculate the centroids of all Wards in London
coordsW <- points_sf_joined%>%
  st_centroid()%>%
  st_geometry()

plot(coordsW, axes=TRUE)

#create a neighbours list
LWard_nb <- points_sf_joined %>%
  poly2nb(., queen=T)

summary(LWard_nb)
#average number of links is 5.88

#plot them
plot(LWard_nb, st_geometry(coordsW), col="red")
#add a map underneath
plot(points_sf_joined$geometry, add=T)
```

```{r Spatial Weight Matrix}
#create a spatial weights matrix from these weights
Lward.lw <- LWard_nb %>%
  nb2mat(., style="B")
#B is binary (neighbour or not)

sum(Lward.lw)

sum(Lward.lw[,1])
```

```{r Spatial Autocorrelation Moran I}
Lward.lw <- LWard_nb %>%
  nb2listw(., style="C")
#style="C" is the global standardisation

# W is row standardised (sums over all links to n)
# C is globally standardised (sums over all links to n)
# U is equal to C divided by the number of neighbours (sums over all links to unity)
# S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n)

#Moran’s I test tells us whether we have clustered values (close to 1) or dispersed values (close to -1)
I_LWard_Global_Density <- points_sf_joined %>%
  pull(density) %>%
  as.vector()%>%
  moran.test(., Lward.lw)

I_LWard_Global_Density

```

The Moran’s I statistic = 0.67 (remember 1 = clustered, 0 = no pattern, -1 = dispersed) which shows that we have some distinctive clustering

```{r Spatial Autocorrelation Geary C}
#Geary’s C tells us whether similar values or dissimilar values are clustering
C_LWard_Global_Density <- points_sf_joined %>%
  pull(density) %>%
  as.vector()%>%
  geary.test(., Lward.lw)

C_LWard_Global_Density

```

The Geary’s C statistic = 0.40 (remember Geary’s C falls between 0 and 2; 1 means no spatial autocorrelation, <1 indicates positive spatial autocorrelation or similar values clustering, >1 indicates negative spatial autocorrelation or dissimilar values clustering) which shows that similar values are clustering

```{r Spatial Autocorrelation Getis-Ord G}
#Getis-Ord G tells us whether high or low values are clustering. 
#if G > Expected: high values clustering 
#if G < expected: low values clustering
G_LWard_Global_Density <- points_sf_joined %>%
  pull(density) %>%
  as.vector()%>%
  globalG.test(., Lward.lw)

G_LWard_Global_Density

```

The General G statistic = G > expected, so high values are tending to cluster

```{r Local Moran I}
#use the localmoran function to generate I for each ward in the city

I_LWard_Local_count <- points_sf_joined %>%
  pull(plaquecount) %>%
  as.vector() %>%
  localmoran(., Lward.lw) %>%
  as_tibble()

I_LWard_Local_Density <- points_sf_joined %>%
  pull(density) %>%
  as.vector() %>%
  localmoran(., Lward.lw) %>%
  as_tibble()

#what does the output (the localMoran object) look like?
slice_head(I_LWard_Local_Density, n=5)
#I score (column 1)
#z-score standard deviation (column 4)

points_sf_joined <- points_sf_joined %>%
  mutate(plaque_count_I = as.numeric(I_LWard_Local_count$Ii))%>%
  mutate(plaque_count_Iz =as.numeric(I_LWard_Local_count$Z.Ii))%>%
  mutate(density_I =as.numeric(I_LWard_Local_Density$Ii))%>%
  mutate(density_Iz =as.numeric(I_LWard_Local_Density$Z.Ii))
```

```{r Plot Local Moran I output}
breaks1 <- c(-1000,-2.58,-1.96,-1.65,1.65,1.96,2.58,1000)

library(RColorBrewer)
MoranColours <- rev(brewer.pal(8, "RdGy"))

#tmap_mode("view")
tm_shape(points_sf_joined) +
  tm_polygons("plaque_count_Iz",
              style="fixed",
              breaks=breaks1,
              palette=MoranColours,
              midpoint=NA,
              title="Local Moran's I, Blue Plaques in London")
```

```{r Local Getis-Ord G}
Gi_LWard_Local_Density <- points_sf_joined %>%
  pull(density) %>%
  as.vector()%>%
  localG(., Lward.lw)

head(Gi_LWard_Local_Density)

?localG
#localG object contains just a single value: the z-score (standardised value relating to whether high values or low values are clustering together)

points_sf_joined <- points_sf_joined %>%
  mutate(density_G = as.numeric(Gi_LWard_Local_Density))

GIColours<- rev(brewer.pal(8, "RdBu"))

# tmap_mode("plot")
tm_shape(points_sf_joined) +
  tm_polygons("density_G",
              style="fixed",
              breaks=breaks1,
              palette=GIColours,
              midpoint=NA,
              title="Gi*, Blue Plaques in London")
```

```{r GCSE scores Local Moran I}
#use head to see what other variables are in the data file
#slice_head(points_sf_joined, n=2)

Datatypelist <- LondonWardsMerged %>% 
  st_drop_geometry()%>%
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")

Datatypelist

I_LWard_Local_GCSE <- LondonWardsMerged %>%
  arrange(GSS_CODE) %>%
  pull(average_gcse_capped_point_scores_2014) %>%
  as.vector() %>%
  localmoran(., Lward.lw) %>%
  as_tibble()

points_sf_joined <- points_sf_joined %>%
  arrange(gss_code) %>%
  mutate(GCSE_LocIz = as.numeric(I_LWard_Local_GCSE$Z.Ii))

tm_shape(points_sf_joined) +
  tm_polygons("GCSE_LocIz",
              style="fixed",
              breaks=breaks1,
              palette=MoranColours,
              midpoint=NA,
              title="Local Moran's I, GCSE Scores")
```

```{r GCSE scores Local Getis-Ord}
G_LWard_Local_GCSE <- LondonWardsMerged %>%
  dplyr::arrange(GSS_CODE) %>%
  dplyr::pull(average_gcse_capped_point_scores_2014) %>%
  as.vector() %>%
  localG(., Lward.lw)

points_sf_joined <- points_sf_joined %>%
  dplyr::arrange(gss_code)%>%
  dplyr::mutate(GCSE_LocGiz = as.numeric(G_LWard_Local_GCSE))

tm_shape(points_sf_joined) +
  tm_polygons("GCSE_LocGiz",
              style="fixed",
              breaks=breaks1,
              palette=GIColours,
              midpoint=NA,
              title="Gi*, GCSE Scores")
```
